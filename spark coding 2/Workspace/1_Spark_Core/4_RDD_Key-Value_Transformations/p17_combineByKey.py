# --------------------------------------------------------
#
# PYTHON PROGRAM DEFINITION
#
# The knowledge a computer has of Python can be specified in 3 levels:
# (1) Prelude knowledge --> The computer has it by default.
# (2) Borrowed knowledge --> The computer gets this knowledge from 3rd party libraries defined by others
#                            (but imported by us in this program).
# (3) Generated knowledge --> The computer gets this knowledge from the new functions defined by us in this program.
#
# When launching in a terminal the command:
# user:~$ python3 this_file.py
# our computer first processes this PYTHON PROGRAM DEFINITION section of the file.
# On it, our computer enhances its Python knowledge from levels (2) and (3) with the imports and new functions
# defined in the program. However, it still does not execute anything.
#
# --------------------------------------------------------

# ------------------------------------------
# IMPORTS
# ------------------------------------------
import pyspark

# ------------------------------------------
# FUNCTION combineByKey_lambda
# ------------------------------------------
def combineByKey_lambda(sc):
    # 1. Operation C1: Creation 'parallelize', so as to store the content of the collection [(1, 2), (3, 8), (1, 4), (3, 4), (3, 6)] into an RDD.

    #         C1: parallelize
    # dataset -----------------> inputRDD

    inputRDD = sc.parallelize([(1, 2), (3, 8), (2, 3), (1, 7), (3, 5), (3, 6), (2, 10)])

    # 2. Operation T1: Transformation 'combineByKey', so as to get a new RDD ('combinedRDD') from inputRDD.

    # combineByKey is a higher-order function.
    # It requires as arguments 3 functions:

    # F1: To be applied in parallel to each node of the cluster.
    # The function is responsible of answering this question:
    # How do you want Spark to process the first (key, value) pair for each key k ?

    # If a node contains 1000 entries (key, value) with key 'k', F1 will only be applied once, for the first (key, value) found.
    # F1 must receive as input 1 parameter: The value of the (key, value) pair.
    # F1 must produce as an output 1 parameter: The accumulator accum generated for the pair (key, accum), created after
    # processing the first (key, value).

    # F2: To be applied in parallel to each node of the cluster.
    # The function is responsible of answering this question:
    # How do you want Spark to process all (key, value) pairs for each key k after having processed the first one and have got an accumulator ?

    # If a node contains 1000 entries (key, value) with key 'k', F2 will be applied 999 times, for all except the first (key, value) found.
    # F2 must receive as input 2 parameters:
    # - The accumulor generated until now.
    # - The value of the new (key, value) pair being found.
    # F2 must produce as an output 1 parameter: The updated accumulator, after aggregating it with the new (key, value) being found.

    # F3: To be applied as a whole single process through all nodes of the cluster.
    # The function is responsible of answering this question:
    # How do you want Spark to process all (key, accumulator) pairs so as to get a whole single (key, accumulator) pair ?

    # If combineByKey is applied to n nodes, F3 will be applied n-1 times, to merge all accumulators under a single accumulator.
    # F3 must receive as input 2 parameters:
    # - The meta-accumulor generated until now.
    # - The accumulator generated by node i, being processed now.
    # F3 must produce as an output 1 parameter: The updated accumulator, after aggregating it with the new (key, accumulator) being found.

    #         C1: parallelize             T1: combineByKey
    # dataset -----------------> inputRDD -----------------> combinedRDD

    combinedRDD = inputRDD.combineByKey(lambda fella: (fella, 1),
                                        lambda accum, new_fella: (accum[0] + new_fella, accum[1] + 1),
                                        lambda final_accum1, final_accum2: (final_accum1[0] + final_accum2[0], final_accum1[1] + final_accum2[1])
                                       )


    # 3. Operation A1: 'collect'.

    #         C1: parallelize             T1: combineByKey               A1: collect
    # dataset -----------------> inputRDD -----------------> combinedRDD ------------> resVAL

    resVAL = combinedRDD.collect()

    # 4. We print by the screen the collection computed in resVAL
    for item in resVAL:
        print(item)


# ------------------------------------------
# FUNCTION createCombiner
# ------------------------------------------
def createCombiner(value):
    # 1. We create the output variable
    res = (value, 1)

    # 2. We return res
    return res


# ------------------------------------------
# FUNCTION mergeValue
# ------------------------------------------
def mergeValue(accum, new_value):
    # 1. We create the output variable
    res = (0, 0)

    # 2. We assign res to the proper value
    val1 = accum[0] + new_value
    val2 = accum[1] + 1
    res = (val1, val2)

    # 3. We return res
    return res


# ------------------------------------------
# FUNCTION mergeCombiners
# ------------------------------------------
def mergeCombiners(accum1, accum2):
    # 1. We create the output variable
    res = (0, 0)

    # 2. We assign res to the proper value
    val1 = accum1[0] + accum2[0]
    val2 = accum1[1] + accum2[1]
    res = (val1, val2)

    # 3. We return res
    return res

# ------------------------------------------
# FUNCTION combineByKey_explicit_function
# ------------------------------------------
def combineByKey_explicit_function(sc):
    # 1. Operation C1: Creation 'parallelize', so as to store the content of the collection [(1, 2), (3, 8), (1, 4), (3, 4), (3, 6)] into an RDD.

    #         C1: parallelize
    # dataset -----------------> inputRDD

    inputRDD = sc.parallelize([(1, 2), (3, 8), (2, 3), (1, 7), (3, 5), (3, 6), (2, 10)])

    # 2. Operation T1: Transformation 'combineByKey', so as to get a new RDD ('combinedRDD') from inputRDD.
    combinedRDD = inputRDD.combineByKey(createCombiner,
                                        mergeValue,
                                        mergeCombiners
                                       )

    # 3. Operation A1: 'collect'.

    #         C1: parallelize             T1: combineByKey               A1: collect
    # dataset -----------------> inputRDD -----------------> combinedRDD ------------> resVAL

    resVAL = combinedRDD.collect()

    # 4. We print by the screen the collection computed in resVAL
    for item in resVAL:
        print(item)

# ------------------------------------------
# FUNCTION function1
# ------------------------------------------
def function1(first_value):
    res = {}

    res[first_value] = 1

    return res

# ------------------------------------------
# FUNCTION function2
# ------------------------------------------
def function2(current_accumulator, new_value):
    res = {}

    for key in current_accumulator:
        res[key] = current_accumulator[key]

    if (new_value in res):
        res[new_value] = res[new_value] + 1
    else:
        res[new_value] = 1

    return res

# ------------------------------------------
# FUNCTION function3
# ------------------------------------------
def function3(accumulator1, accumulator2):
    res = {}

    for key in accumulator1:
        res[key] = accumulator1[key]

    for key in accumulator2:
        if (key in res):
            res[key] = res[key] + accumulator2[key]
        else:
            res[key] = accumulator2[key]

    return res

# ------------------------------------------
# FUNCTION combineByKey_complex
# ------------------------------------------
def combineByKey_complex(sc):
    # 1. Operation C1: Creation 'parallelize', so as to store the content of the collection [(1, 2), (3, 8), (1, 4), (3, 4), (3, 6)] into an RDD.

    #         C1: parallelize
    # dataset -----------------> inputRDD
    inputRDD = sc.parallelize([("Hello", 1),
                               ("Hello", 2),
                               ("Goodbye", 2),
                               ("Hello", 3),
                               ("Hello", 1),
                               ("Hello", 1),
                               ("Goodbye", 2),
                               ("Hello", 3)
                              ])

    # 2. Operation T1: Transformation 'combineByKey', so as to get a new RDD ('combinedRDD') from inputRDD.

    #         C1: parallelize             T1: combineByKey
    # dataset -----------------> inputRDD -----------------> combinedRDD

    combinedRDD = inputRDD.combineByKey( function1, function2, function3 )

    # 3. Operation A1: 'collect'.

    #         C1: parallelize             T1: combineByKey               A1: collect
    # dataset -----------------> inputRDD -----------------> combinedRDD ------------> resVAL
    resVAL = combinedRDD.collect()

    # 4. We print by the screen the collection computed in resVAL
    for my_tuple in resVAL:
        for key in my_tuple[1]:
            print(str(my_tuple[0]) + " -> " + str(key) + " : " + str(my_tuple[1][key]))

# ------------------------------------------
# FUNCTION my_main
# ------------------------------------------
def my_main(sc):
    print("\n\n--- [BLOCK 1] combineByKey with F1, F2, F3 defined via a lambda expressions ---")
    combineByKey_lambda(sc)

    print("\n\n--- [BLOCK 2] combineByKey with F1, F2, F3 defined via a explicit functions ---")
    combineByKey_explicit_function(sc)

    print("\n\n--- [BLOCK 3] combineByKey with a more complicated functionality ---")
    combineByKey_complex(sc)


# --------------------------------------------------------
#
# PYTHON PROGRAM EXECUTION
#
# Once our computer has finished processing the PYTHON PROGRAM DEFINITION section its knowledge is set.
# Now its time to apply this knowledge.
#
# When launching in a terminal the command:
# user:~$ python3 this_file.py
# our computer finally processes this PYTHON PROGRAM EXECUTION section, which:
# (i) Specifies the function F to be executed.
# (ii) Define any input parameter such this function F has to be called with.
#
# --------------------------------------------------------
if __name__ == '__main__':
    # 1. We use as many input arguments as needed
    pass

    # 2. Local or Databricks
    pass

    # 3. We configure the Spark Context
    sc = pyspark.SparkContext.getOrCreate()
    sc.setLogLevel('WARN')
    print("\n\n\n")

    # 4. We call to my_main
    my_main(sc)
